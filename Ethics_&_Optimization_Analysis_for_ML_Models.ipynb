{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sdTRXa8YXyrf",
        "outputId": "7b37a0ea-69a2-45f7-d734-c37c65520aa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ETHICS & OPTIMIZATION ANALYSIS FOR ML MODELS\n",
            "================================================================================\n",
            "\n",
            "1. ETHICAL CONSIDERATIONS\n",
            "--------------------------------------------------\n",
            "\n",
            "A. POTENTIAL BIASES IN MNIST MODEL:\n",
            "   • Demographic bias: MNIST contains handwritten digits from specific\n",
            "     populations and may not generalize to different writing styles\n",
            "   • Cultural bias: Different cultures may write digits differently\n",
            "   • Age bias: Children vs adults have different handwriting patterns\n",
            "   • Quality bias: Dataset may favor certain image qualities/conditions\n",
            "   • Selection bias: Historical data collection methods may not be representative\n",
            "\n",
            "B. POTENTIAL BIASES IN AMAZON REVIEWS MODEL:\n",
            "   • Language bias: English-only reviews exclude non-English speakers\n",
            "   • Socioeconomic bias: Reviews may favor products accessible to certain income levels\n",
            "   • Platform bias: Amazon-specific user behavior patterns\n",
            "   • Temporal bias: Reviews from different time periods may have different patterns\n",
            "   • Product category bias: Some categories may have more polarized reviews\n",
            "   • Demographic bias: Age, gender, and cultural differences in review writing\n",
            "\n",
            "C. BIAS ANALYSIS EXAMPLE - Amazon Reviews:\n",
            "\n",
            "Bias Analysis by Price Range:\n",
            "             mean  count   std\n",
            "price_range                   \n",
            "high          5.0      2  0.00\n",
            "low           2.0      2  1.41\n",
            "medium        4.0      1   NaN\n",
            "\n",
            "\n",
            "D. BIAS MITIGATION STRATEGIES:\n",
            "----------------------------------------\n",
            "\n",
            "1. DATA-LEVEL MITIGATION:\n",
            "   • Data Augmentation: Increase diversity in training data\n",
            "   • Balanced Sampling: Ensure equal representation across groups\n",
            "   • Synthetic Data Generation: Create balanced synthetic samples\n",
            "   • Multi-source Data Collection: Gather data from diverse sources\n",
            "\n",
            "2. ALGORITHM-LEVEL MITIGATION:\n",
            "   • Fairness Constraints: Add fairness objectives to loss functions\n",
            "   • Adversarial Debiasing: Train models to be invariant to protected attributes\n",
            "   • Regularization: Penalize discriminatory patterns\n",
            "\n",
            "3. POST-PROCESSING MITIGATION:\n",
            "   • Threshold Optimization: Adjust decision thresholds per group\n",
            "   • Calibration: Ensure equal calibration across groups\n",
            "   • Output Adjustment: Modify predictions to achieve fairness\n",
            "\n",
            "4. TENSORFLOW FAIRNESS INDICATORS EXAMPLE:\n",
            "   (Simulated metrics for demonstration)\n",
            "\n",
            "Fairness Metrics by Group:\n",
            "         Accuracy  Precision  Recall  F1-Score  False Positive Rate  \\\n",
            "Group A      0.85       0.83    0.87      0.85                 0.12   \n",
            "Group B      0.78       0.75    0.82      0.78                 0.18   \n",
            "Group C      0.82       0.80    0.85      0.82                 0.15   \n",
            "\n",
            "         False Negative Rate  \n",
            "Group A                 0.13  \n",
            "Group B                 0.18  \n",
            "Group C                 0.15  \n",
            "\n",
            "Fairness Gaps (Max - Min):\n",
            "   Accuracy: 0.070\n",
            "   Precision: 0.080\n",
            "   Recall: 0.050\n",
            "   F1-Score: 0.070\n",
            "   False Positive Rate: 0.060\n",
            "   False Negative Rate: 0.050\n",
            "\n",
            "\n",
            "2. TROUBLESHOOTING CHALLENGE - BUGGY CODE\n",
            "--------------------------------------------------\n",
            "\n",
            "ORIGINAL BUGGY CODE:\n",
            "--------------------\n",
            "\n",
            "# BUGGY CODE - DO NOT RUN\n",
            "import tensorflow as tf\n",
            "from tensorflow.keras import layers\n",
            "\n",
            "# Bug 1: Wrong input shape\n",
            "model = tf.keras.Sequential([\n",
            "    layers.Dense(128, activation='relu', input_shape=(28, 28)),  # BUG: Should be (784,) for flattened\n",
            "    layers.Dense(64, activation='relu'),\n",
            "    layers.Dense(10, activation='softmax')\n",
            "])\n",
            "\n",
            "# Bug 2: Wrong loss function for multiclass\n",
            "model.compile(optimizer='adam',\n",
            "              loss='binary_crossentropy',  # BUG: Should be 'sparse_categorical_crossentropy'\n",
            "              metrics=['accuracy'])\n",
            "\n",
            "# Bug 3: Dimension mismatch in data\n",
            "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
            "x_train = x_train / 255.0  # BUG: Not flattened for Dense layers\n",
            "x_test = x_test / 255.0\n",
            "\n",
            "# Bug 4: Wrong batch dimension in fit\n",
            "model.fit(x_train, y_train, epochs=5, batch_size=32)  # Will fail due to shape mismatch\n",
            "\n",
            "\n",
            "IDENTIFIED BUGS:\n",
            "---------------\n",
            "1. Input shape mismatch: Dense layer expects flattened input (784,), not (28,28)\n",
            "2. Wrong loss function: binary_crossentropy for multiclass problem\n",
            "3. Data not flattened: Images need to be reshaped for Dense layers\n",
            "4. Potential validation data missing\n",
            "5. No error handling or data validation\n",
            "\n",
            "FIXED CODE:\n",
            "----------\n",
            "Creating and training corrected model...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Training data shape: (60000, 28, 28)\n",
            "Training labels shape: (60000,)\n",
            "Test data shape: (10000, 28, 28)\n",
            "Test labels shape: (10000,)\n",
            "Label range: 0 to 9\n",
            "\n",
            "Model Summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m100,480\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m109,386\u001b[0m (427.29 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,386</span> (427.29 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m109,386\u001b[0m (427.29 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,386</span> (427.29 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model...\n",
            "Epoch 1/3\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.7890 - loss: 0.7157 - val_accuracy: 0.9585 - val_loss: 0.1458\n",
            "Epoch 2/3\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9423 - loss: 0.1944 - val_accuracy: 0.9712 - val_loss: 0.1029\n",
            "Epoch 3/3\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9612 - loss: 0.1321 - val_accuracy: 0.9742 - val_loss: 0.0904\n",
            "\n",
            "Test accuracy: 0.9692\n",
            "Test loss: 0.0998\n",
            "\n",
            "\n",
            "3. DEBUGGING BEST PRACTICES:\n",
            "------------------------------\n",
            "   1. Check data shapes at each step\n",
            "   2. Validate input/output dimensions match model expectations\n",
            "   3. Use appropriate loss functions for the problem type\n",
            "   4. Add data type and range validation\n",
            "   5. Include proper error handling and logging\n",
            "   6. Use callbacks for monitoring (EarlyStopping, ModelCheckpoint)\n",
            "   7. Visualize data and predictions to catch issues\n",
            "   8. Start with simple models and gradually increase complexity\n",
            "   9. Use TensorBoard for training visualization\n",
            "   10. Test with small datasets first\n",
            "\n",
            "\n",
            "4. SPACY RULE-BASED BIAS MITIGATION:\n",
            "----------------------------------------\n",
            "\n",
            "Strategies for reducing bias in NLP models:\n",
            "1. Diverse Training Data:\n",
            "   • Include reviews from different demographics\n",
            "   • Balance across product categories and price ranges\n",
            "   • Include multiple languages and dialects\n",
            "\n",
            "2. Bias-Aware Rule Creation:\n",
            "   • Avoid rules that favor specific groups\n",
            "   • Test rules across different populations\n",
            "   • Use inclusive language patterns\n",
            "\n",
            "3. Evaluation Across Subgroups:\n",
            "   • Measure performance on different demographic groups\n",
            "   • Check for disparate impact\n",
            "   • Monitor for systematic errors\n",
            "\n",
            "4. Example: Bias-Aware Sentiment Rules\n",
            "\n",
            "Bias-Aware Sentiment Analysis Examples:\n",
            "   'The product is not bad' → negative (score: -0.7)\n",
            "   'It's okay, could be better' → negative (score: -0.1)\n",
            "   'Excellent quality and great value' → positive (score: 2)\n",
            "\n",
            "================================================================================\n",
            "ETHICS & OPTIMIZATION ANALYSIS COMPLETE\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Ethics & Optimization Analysis for ML Models\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ETHICS & OPTIMIZATION ANALYSIS FOR ML MODELS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# PART 1: ETHICAL CONSIDERATIONS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n1. ETHICAL CONSIDERATIONS\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"\\nA. POTENTIAL BIASES IN MNIST MODEL:\")\n",
        "print(\"   • Demographic bias: MNIST contains handwritten digits from specific\")\n",
        "print(\"     populations and may not generalize to different writing styles\")\n",
        "print(\"   • Cultural bias: Different cultures may write digits differently\")\n",
        "print(\"   • Age bias: Children vs adults have different handwriting patterns\")\n",
        "print(\"   • Quality bias: Dataset may favor certain image qualities/conditions\")\n",
        "print(\"   • Selection bias: Historical data collection methods may not be representative\")\n",
        "\n",
        "print(\"\\nB. POTENTIAL BIASES IN AMAZON REVIEWS MODEL:\")\n",
        "print(\"   • Language bias: English-only reviews exclude non-English speakers\")\n",
        "print(\"   • Socioeconomic bias: Reviews may favor products accessible to certain income levels\")\n",
        "print(\"   • Platform bias: Amazon-specific user behavior patterns\")\n",
        "print(\"   • Temporal bias: Reviews from different time periods may have different patterns\")\n",
        "print(\"   • Product category bias: Some categories may have more polarized reviews\")\n",
        "print(\"   • Demographic bias: Age, gender, and cultural differences in review writing\")\n",
        "\n",
        "# Sample bias analysis for Amazon reviews\n",
        "print(\"\\nC. BIAS ANALYSIS EXAMPLE - Amazon Reviews:\")\n",
        "\n",
        "# Create sample data with potential biases\n",
        "biased_reviews = [\n",
        "    {'text': 'This luxury product is amazing!', 'rating': 5, 'price_range': 'high', 'demographic': 'affluent'},\n",
        "    {'text': 'Cheap quality, not worth it', 'rating': 1, 'price_range': 'low', 'demographic': 'budget'},\n",
        "    {'text': 'Great value for money', 'rating': 4, 'price_range': 'medium', 'demographic': 'middle'},\n",
        "    {'text': 'Premium features work perfectly', 'rating': 5, 'price_range': 'high', 'demographic': 'affluent'},\n",
        "    {'text': 'Basic functionality is okay', 'rating': 3, 'price_range': 'low', 'demographic': 'budget'},\n",
        "]\n",
        "\n",
        "bias_df = pd.DataFrame(biased_reviews)\n",
        "print(\"\\nBias Analysis by Price Range:\")\n",
        "bias_analysis = bias_df.groupby('price_range')['rating'].agg(['mean', 'count', 'std']).round(2)\n",
        "print(bias_analysis)\n",
        "\n",
        "# ============================================================================\n",
        "# BIAS MITIGATION STRATEGIES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\\nD. BIAS MITIGATION STRATEGIES:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "print(\"\\n1. DATA-LEVEL MITIGATION:\")\n",
        "print(\"   • Data Augmentation: Increase diversity in training data\")\n",
        "print(\"   • Balanced Sampling: Ensure equal representation across groups\")\n",
        "print(\"   • Synthetic Data Generation: Create balanced synthetic samples\")\n",
        "print(\"   • Multi-source Data Collection: Gather data from diverse sources\")\n",
        "\n",
        "print(\"\\n2. ALGORITHM-LEVEL MITIGATION:\")\n",
        "print(\"   • Fairness Constraints: Add fairness objectives to loss functions\")\n",
        "print(\"   • Adversarial Debiasing: Train models to be invariant to protected attributes\")\n",
        "print(\"   • Regularization: Penalize discriminatory patterns\")\n",
        "\n",
        "print(\"\\n3. POST-PROCESSING MITIGATION:\")\n",
        "print(\"   • Threshold Optimization: Adjust decision thresholds per group\")\n",
        "print(\"   • Calibration: Ensure equal calibration across groups\")\n",
        "print(\"   • Output Adjustment: Modify predictions to achieve fairness\")\n",
        "\n",
        "# Example: TensorFlow Fairness Indicators simulation\n",
        "print(\"\\n4. TENSORFLOW FAIRNESS INDICATORS EXAMPLE:\")\n",
        "print(\"   (Simulated metrics for demonstration)\")\n",
        "\n",
        "# Simulate fairness metrics\n",
        "groups = ['Group A', 'Group B', 'Group C']\n",
        "metrics = {\n",
        "    'Accuracy': [0.85, 0.78, 0.82],\n",
        "    'Precision': [0.83, 0.75, 0.80],\n",
        "    'Recall': [0.87, 0.82, 0.85],\n",
        "    'F1-Score': [0.85, 0.78, 0.82],\n",
        "    'False Positive Rate': [0.12, 0.18, 0.15],\n",
        "    'False Negative Rate': [0.13, 0.18, 0.15]\n",
        "}\n",
        "\n",
        "fairness_df = pd.DataFrame(metrics, index=groups)\n",
        "print(\"\\nFairness Metrics by Group:\")\n",
        "print(fairness_df.round(3))\n",
        "\n",
        "# Calculate fairness gaps\n",
        "print(\"\\nFairness Gaps (Max - Min):\")\n",
        "for metric in metrics.keys():\n",
        "    gap = max(metrics[metric]) - min(metrics[metric])\n",
        "    print(f\"   {metric}: {gap:.3f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# PART 2: TROUBLESHOOTING CHALLENGE - BUGGY TENSORFLOW CODE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\\n2. TROUBLESHOOTING CHALLENGE - BUGGY CODE\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"\\nORIGINAL BUGGY CODE:\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "buggy_code = '''\n",
        "# BUGGY CODE - DO NOT RUN\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Bug 1: Wrong input shape\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(128, activation='relu', input_shape=(28, 28)),  # BUG: Should be (784,) for flattened\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Bug 2: Wrong loss function for multiclass\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',  # BUG: Should be 'sparse_categorical_crossentropy'\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Bug 3: Dimension mismatch in data\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train / 255.0  # BUG: Not flattened for Dense layers\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Bug 4: Wrong batch dimension in fit\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32)  # Will fail due to shape mismatch\n",
        "'''\n",
        "\n",
        "print(buggy_code)\n",
        "\n",
        "print(\"\\nIDENTIFIED BUGS:\")\n",
        "print(\"-\" * 15)\n",
        "print(\"1. Input shape mismatch: Dense layer expects flattened input (784,), not (28,28)\")\n",
        "print(\"2. Wrong loss function: binary_crossentropy for multiclass problem\")\n",
        "print(\"3. Data not flattened: Images need to be reshaped for Dense layers\")\n",
        "print(\"4. Potential validation data missing\")\n",
        "print(\"5. No error handling or data validation\")\n",
        "\n",
        "print(\"\\nFIXED CODE:\")\n",
        "print(\"-\" * 10)\n",
        "\n",
        "# Demonstrate the fixed code\n",
        "print(\"Creating and training corrected model...\")\n",
        "\n",
        "# Fixed implementation\n",
        "def create_fixed_model():\n",
        "    \"\"\"Create a properly configured MNIST model\"\"\"\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Flatten(input_shape=(28, 28)),  # FIX: Properly flatten input\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.2),  # IMPROVEMENT: Add dropout for regularization\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # FIX: Correct loss function for multiclass classification\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',  # FIXED\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Load and preprocess data correctly\n",
        "def load_and_preprocess_data():\n",
        "    \"\"\"Load and properly preprocess MNIST data\"\"\"\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "    # Normalize pixel values\n",
        "    x_train = x_train.astype('float32') / 255.0\n",
        "    x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "    # Validation: Check data shapes and types\n",
        "    print(f\"Training data shape: {x_train.shape}\")\n",
        "    print(f\"Training labels shape: {y_train.shape}\")\n",
        "    print(f\"Test data shape: {x_test.shape}\")\n",
        "    print(f\"Test labels shape: {y_test.shape}\")\n",
        "    print(f\"Label range: {y_train.min()} to {y_train.max()}\")\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "# Create and train the fixed model\n",
        "try:\n",
        "    model = create_fixed_model()\n",
        "    (x_train, y_train), (x_test, y_test) = load_and_preprocess_data()\n",
        "\n",
        "    print(\"\\nModel Summary:\")\n",
        "    model.summary()\n",
        "\n",
        "    print(\"\\nTraining model...\")\n",
        "    # FIX: Add validation data and proper callbacks\n",
        "    history = model.fit(\n",
        "        x_train, y_train,\n",
        "        epochs=3,  # Reduced for demo\n",
        "        batch_size=128,\n",
        "        validation_split=0.1,  # IMPROVEMENT: Add validation\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate model\n",
        "    test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(f\"\\nTest accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"Test loss: {test_loss:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during model training: {e}\")\n",
        "\n",
        "# ============================================================================\n",
        "# ADDITIONAL DEBUGGING TECHNIQUES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\\n3. DEBUGGING BEST PRACTICES:\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "debugging_practices = [\n",
        "    \"1. Check data shapes at each step\",\n",
        "    \"2. Validate input/output dimensions match model expectations\",\n",
        "    \"3. Use appropriate loss functions for the problem type\",\n",
        "    \"4. Add data type and range validation\",\n",
        "    \"5. Include proper error handling and logging\",\n",
        "    \"6. Use callbacks for monitoring (EarlyStopping, ModelCheckpoint)\",\n",
        "    \"7. Visualize data and predictions to catch issues\",\n",
        "    \"8. Start with simple models and gradually increase complexity\",\n",
        "    \"9. Use TensorBoard for training visualization\",\n",
        "    \"10. Test with small datasets first\"\n",
        "]\n",
        "\n",
        "for practice in debugging_practices:\n",
        "    print(f\"   {practice}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SPACY RULE-BASED BIAS MITIGATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\\n4. SPACY RULE-BASED BIAS MITIGATION:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "print(\"\\nStrategies for reducing bias in NLP models:\")\n",
        "print(\"1. Diverse Training Data:\")\n",
        "print(\"   • Include reviews from different demographics\")\n",
        "print(\"   • Balance across product categories and price ranges\")\n",
        "print(\"   • Include multiple languages and dialects\")\n",
        "\n",
        "print(\"\\n2. Bias-Aware Rule Creation:\")\n",
        "print(\"   • Avoid rules that favor specific groups\")\n",
        "print(\"   • Test rules across different populations\")\n",
        "print(\"   • Use inclusive language patterns\")\n",
        "\n",
        "print(\"\\n3. Evaluation Across Subgroups:\")\n",
        "print(\"   • Measure performance on different demographic groups\")\n",
        "print(\"   • Check for disparate impact\")\n",
        "print(\"   • Monitor for systematic errors\")\n",
        "\n",
        "# Example bias mitigation in rule-based sentiment\n",
        "print(\"\\n4. Example: Bias-Aware Sentiment Rules\")\n",
        "\n",
        "class BiasAwareSentimentAnalyzer:\n",
        "    def __init__(self):\n",
        "        # Inclusive positive words across cultures\n",
        "        self.positive_words = {\n",
        "            'excellent', 'good', 'great', 'amazing', 'wonderful',\n",
        "            'satisfactory', 'decent', 'adequate', 'acceptable', 'fine'\n",
        "        }\n",
        "\n",
        "        # Avoid culturally biased negative terms\n",
        "        self.negative_words = {\n",
        "            'poor', 'bad', 'disappointing', 'inadequate', 'unsatisfactory',\n",
        "            'defective', 'broken', 'faulty', 'substandard'\n",
        "        }\n",
        "\n",
        "        # Cultural sensitivity adjustments\n",
        "        self.cultural_adjustments = {\n",
        "            'not bad': 0.3,  # Some cultures use double negatives positively\n",
        "            'could be better': -0.2,  # Indirect criticism\n",
        "            'okay': 0.1  # Neutral but slightly positive in some contexts\n",
        "        }\n",
        "\n",
        "    def analyze_with_bias_awareness(self, text):\n",
        "        \"\"\"Analyze sentiment with bias mitigation\"\"\"\n",
        "        # Implementation would include cultural context awareness\n",
        "        # This is a simplified example\n",
        "        score = 0\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Check for cultural phrases\n",
        "        for phrase, adjustment in self.cultural_adjustments.items():\n",
        "            if phrase in text_lower:\n",
        "                score += adjustment\n",
        "\n",
        "        # Regular sentiment analysis\n",
        "        words = text_lower.split()\n",
        "        for word in words:\n",
        "            if word in self.positive_words:\n",
        "                score += 1\n",
        "            elif word in self.negative_words:\n",
        "                score -= 1\n",
        "\n",
        "        return {\n",
        "            'score': score,\n",
        "            'sentiment': 'positive' if score > 0 else 'negative' if score < 0 else 'neutral',\n",
        "            'bias_adjusted': True\n",
        "        }\n",
        "\n",
        "# Demonstrate bias-aware analysis\n",
        "bias_aware_analyzer = BiasAwareSentimentAnalyzer()\n",
        "test_phrases = [\n",
        "    \"The product is not bad\",\n",
        "    \"It's okay, could be better\",\n",
        "    \"Excellent quality and great value\"\n",
        "]\n",
        "\n",
        "print(\"\\nBias-Aware Sentiment Analysis Examples:\")\n",
        "for phrase in test_phrases:\n",
        "    result = bias_aware_analyzer.analyze_with_bias_awareness(phrase)\n",
        "    print(f\"   '{phrase}' → {result['sentiment']} (score: {result['score']})\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ETHICS & OPTIMIZATION ANALYSIS COMPLETE\")\n",
        "print(\"=\" * 80)"
      ]
    }
  ]
}